{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNCKlCXP3xWkCtEdMiwGFdy",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Graceyong1020/DeepLearning_Detection/blob/main/Faster_rcnn_baseline.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "uKbwXhDx6w1g",
        "outputId": "1498215e-e1f4-4226-c5e9-c54c31fbf033"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting torchnet\n",
            "  Downloading torchnet-0.0.4.tar.gz (23 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (from torchnet) (2.5.1+cu121)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.11/dist-packages (from torchnet) (1.17.0)\n",
            "Collecting visdom (from torchnet)\n",
            "  Downloading visdom-0.2.4.tar.gz (1.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.4/1.4 MB\u001b[0m \u001b[31m17.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch->torchnet) (3.16.1)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.11/dist-packages (from torch->torchnet) (4.12.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch->torchnet) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch->torchnet) (3.1.5)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch->torchnet) (2024.10.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.11/dist-packages (from torch->torchnet) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.11/dist-packages (from torch->torchnet) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.11/dist-packages (from torch->torchnet) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch->torchnet) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.11/dist-packages (from torch->torchnet) (12.1.3.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.11/dist-packages (from torch->torchnet) (11.0.2.54)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.11/dist-packages (from torch->torchnet) (10.3.2.106)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.11/dist-packages (from torch->torchnet) (11.4.5.107)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.11/dist-packages (from torch->torchnet) (12.1.0.106)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch->torchnet) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.11/dist-packages (from torch->torchnet) (12.1.105)\n",
            "Requirement already satisfied: triton==3.1.0 in /usr/local/lib/python3.11/dist-packages (from torch->torchnet) (3.1.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch->torchnet) (1.13.1)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.11/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch->torchnet) (12.6.85)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch->torchnet) (1.3.0)\n",
            "Requirement already satisfied: numpy>=1.8 in /usr/local/lib/python3.11/dist-packages (from visdom->torchnet) (1.26.4)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (from visdom->torchnet) (1.13.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from visdom->torchnet) (2.32.3)\n",
            "Requirement already satisfied: tornado in /usr/local/lib/python3.11/dist-packages (from visdom->torchnet) (6.3.3)\n",
            "Requirement already satisfied: jsonpatch in /usr/local/lib/python3.11/dist-packages (from visdom->torchnet) (1.33)\n",
            "Requirement already satisfied: websocket-client in /usr/local/lib/python3.11/dist-packages (from visdom->torchnet) (1.8.0)\n",
            "Requirement already satisfied: pillow in /usr/local/lib/python3.11/dist-packages (from visdom->torchnet) (11.1.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch->torchnet) (3.0.2)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.11/dist-packages (from jsonpatch->visdom->torchnet) (3.0.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->visdom->torchnet) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->visdom->torchnet) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->visdom->torchnet) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->visdom->torchnet) (2024.12.14)\n",
            "Building wheels for collected packages: torchnet, visdom\n",
            "  Building wheel for torchnet (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for torchnet: filename=torchnet-0.0.4-py3-none-any.whl size=29729 sha256=c78b350ac07d423ebd4e185e69511316133e9bd915907e659b642ae216c07815\n",
            "  Stored in directory: /root/.cache/pip/wheels/cd/b2/52/10719791e6bf529f41a487694816432200fadfd90dfcabd0bb\n",
            "  Building wheel for visdom (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for visdom: filename=visdom-0.2.4-py3-none-any.whl size=1408196 sha256=e77634ac3d9447006313c4121bcd28962d5bc2e1f9c2a162a7d965903f803dce\n",
            "  Stored in directory: /root/.cache/pip/wheels/fa/a4/bb/2be445c295d88a74f9c0a4232f04860ca489a5c7c57eb959d9\n",
            "Successfully built torchnet visdom\n",
            "Installing collected packages: visdom, torchnet\n",
            "Successfully installed torchnet-0.0.4 visdom-0.2.4\n"
          ]
        }
      ],
      "source": [
        "!pip install torchnet"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import warnings\n",
        "warnings.filterwarnings(action='ignore')\n",
        "\n",
        "import os\n",
        "import six\n",
        "from collections import namedtuple\n",
        "\n",
        "import cv2\n",
        "import albumentations as A\n",
        "from albumentations.pytorch import ToTensorV2\n",
        "# COCO formate의 데이터셋 사용을 돕는 라이브러리\n",
        "from pycocotools.coco import COCO\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from tqdm import tqdm\n",
        "\n",
        "# torchvision - computer vision용 pytorch 라이브러리\n",
        "from torchvision.models import vgg16\n",
        "from torchvision.ops import RoIPool\n",
        "from torchvision.ops import nms\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn import functional as F\n",
        "from torch.utils.data import Dataset\n",
        "from torch.utils import data as data_\n",
        "\n",
        "# torchnet - logging, eval, visualize 등을 돕는 라이브러리\n",
        "from torchnet.meter import ConfusionMeter, AverageValueMeter"
      ],
      "metadata": {
        "id": "OJ0Do3Lh6yNc"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Util Functions"
      ],
      "metadata": {
        "id": "1hBcodXN7QWh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# loc2bbox 함수: 소스 바운딩 박스(src_bbox)를 위치 델타(loc)를 사용하여 목적지 바운딩 박스(dst_bbox)로 변환\n",
        "\n",
        "\n",
        "def loc2bbox(src_bbox, loc):\n",
        "    \"\"\"\n",
        "    from src_bbox to dst bbox using loc\n",
        "    Args:\n",
        "        src_bbox: 소스 바운딩 박스\n",
        "        loc: 델타\n",
        "    Returns: dst_bbox\n",
        "    \"\"\"\n",
        "\n",
        "    if src_bbox.shape[0] == 0: # 소스 바운딩 박스가 비어있으면\n",
        "        return np.zeros((0, 4), dtype=loc.dtype) # 빈 배열 반환\n",
        "\n",
        "    src_bbox = src_bbox.astype(src_bbox.dtype, copy=False) # 소스 바운딩 박스의 데이터 타입 유지\n",
        "\n",
        "    # x_min, y_min, x_max, y_max\n",
        "    src_height = src_bbox[:, 2] - src_bbox[:, 0]\n",
        "    src_width = src_bbox[:, 3] - src_bbox[:, 1]\n",
        "    src_ctr_y = src_bbox[:, 0] + 0.5 * src_height\n",
        "    src_ctr_x = src_bbox[:, 1] + 0.5 * src_width\n",
        "\n",
        "    # 델타 계산\n",
        "    dy = loc[:, 0::4]\n",
        "    dx = loc[:, 1::4]\n",
        "    dh = loc[:, 2::4]\n",
        "    dw = loc[:, 3::4]\n",
        "\n",
        "    ctr_y = dy * src_height[:, np.newaxis] + src_ctr_y[:, np.newaxis] # 새로운 중심 y 좌표 계산\n",
        "    ctr_x = dx * src_width[:, np.newaxis] + src_ctr_x[:, np.newaxis] # 새로운 중심 x 좌표 계산\n",
        "    h = np.exp(dh) * src_height[:, np.newaxis] # 새로운 높이 계산\n",
        "    w = np.exp(dw) * src_width[:, np.newaxis] # 새로운 너비 계산\n",
        "\n",
        "\n",
        "    dst_bbox = np.zeros(loc.shape, dtype=loc.dtype) # 목적지 바운딩 박스 초기화\n",
        "    dst_bbox[:, 0::4] = ctr_y - 0.5 * h # 새로운 y_min 계산\n",
        "    dst_bbox[:, 1::4] = ctr_x - 0.5 * w # 새로운 x_min 계산\n",
        "    dst_bbox[:, 2::4] = ctr_y + 0.5 * h # 새로운 y_max 계산\n",
        "    dst_bbox[:, 3::4] = ctr_x + 0.5 * w # 새로운 x_max 계산\n",
        "\n",
        "    return dst_bbox # 목적지 바운딩 박스 반환\n",
        "\n",
        "\n",
        "# bbox2loc: 바운딩 박스 위치 델타로 변환\n",
        "def bbox2loc(src_bbox, dst_bbox):\n",
        "    \"\"\"\n",
        "    src_bbox : 예측된 좌표값(or anchor), dst_bbox: gt 좌표값 -> loc(y, x, h, w)\n",
        "    \"\"\"\n",
        "\n",
        "    # x_min, y_min, x_max, y_max\n",
        "    height = src_bbox[:, 2] - src_bbox[:, 0]\n",
        "    width = src_bbox[:, 3] - src_bbox[:, 1]\n",
        "    ctr_y = src_bbox[:, 0] + 0.5 * height\n",
        "    ctr_x = src_bbox[:, 1] + 0.5 * width\n",
        "\n",
        "    # 목적지 계산. x_min, y_min, x_max, y_max\n",
        "    base_height = dst_bbox[:, 2] - dst_bbox[:, 0]\n",
        "    base_width = dst_bbox[:, 3] - dst_bbox[:, 1]\n",
        "    base_ctr_y = dst_bbox[:, 0] + 0.5 * base_height\n",
        "    base_ctr_x = dst_bbox[:, 1] + 0.5 * base_width\n",
        "\n",
        "    eps = np.finfo(height.dtype).eps  # 작은 값 설정\n",
        "    height = np.maximum(height, eps) # 높이가 0이 되지 않도록 설정\n",
        "    width = np.maximum(width, eps) # 너비가 0이 되지 않도록 설정\n",
        "\n",
        "    # 델타 x,y,w,h 계산\n",
        "    dy = (base_ctr_y - ctr_y) / height\n",
        "    dx = (base_ctr_x - ctr_x) / width\n",
        "    dh = np.log(base_height / height)\n",
        "    dw = np.log(base_width / width)\n",
        "\n",
        "    loc = np.vstack((dy, dx, dh, dw)).transpose() # 델타 병합\n",
        "    return loc # 델타 변환\n",
        "\n",
        "\n",
        "# normal_init: 가중치 초기화\n",
        "def normal_init(m, mean, stddev, truncated=False):\n",
        "    \"\"\"\n",
        "    weight initialization\n",
        "    \"\"\"\n",
        "    if truncated: # 절단된 초기화인 경우\n",
        "        m.weight.data.normal_().fmod_(2).mul_(stddev).add_(mean) # 절단된 정규분포로 초기화\n",
        "    else:\n",
        "        m.weight.data.normal_(mean, stddev)  # 정규분포로 초기화\n",
        "        m.bias.data.zero_() # 바이어스 0으로 초기화\n",
        "\n",
        "# get_inside_index:  이미지 내부에 완전히 위치한 앵커의 인덱스를 계산\n",
        "def get_inside_index(anchor, H, W):\n",
        "    # Calc indicies of anchors which are located completely inside of the image\n",
        "    # whose size is speficied.\n",
        "    index_inside = np.where(\n",
        "        (anchor[:, 0] >= 0) &  # y_min >= 0\n",
        "        (anchor[:, 1] >= 0) &  # x_min >= 0\n",
        "        (anchor[:, 2] <= H) &  # y_max <= H\n",
        "        (anchor[:, 3] <= W)    # x_max <= W\n",
        "    )[0]\n",
        "    return index_inside # 인덱스 반환\n",
        "\n",
        "# unmap: 데이터의 하위 집합을 원래의 데이터 집합으로 다시 매핑\n",
        "def unmap(data, count, index, fill=0):\n",
        "    # Unmap a subset of item (data) back to the original set of items (of size count)\n",
        "    if len(data.shape) == 1: # 1차원 배열인 경우\n",
        "        ret = np.empty((count,), dtype=data.dtype) # 빈 배열 생성\n",
        "        ret.fill(fill) # fill 값으로 채움\n",
        "        ret[index] = data # 인덱스에 데이터 매핑\n",
        "    else: # 다차원 배열인 경우\n",
        "        ret = np.empty((count,) + data.shape[1:], dtype=data.dtype) # 빈 배열 생성\n",
        "        ret.fill(fill) # fill 값으로 채움\n",
        "        ret[index, :] = data # 인덱스에 데이터 매핑\n",
        "    return ret # 매핑된 배열 반환\n",
        "\n",
        "\n",
        "## util: 데이터를 다양한 형식으로 변환 ##\n",
        "def tonumpy(data):\n",
        "    if isinstance(data, np.ndarray): # 데이터가 numpy 배열인 경우\n",
        "        return data\n",
        "    if isinstance(data, torch.Tensor): # 데이터가 torch 텐서인 경우\n",
        "        return data.detach().cpu().numpy()\n",
        "\n",
        "def totensor(data, cuda = True):\n",
        "    if isinstance(data, np.ndarray):\n",
        "        tensor = torch.from_numpy(data)\n",
        "    if isinstance(data, torch.Tensor):\n",
        "        tensor = data.detach() # 텐서 분리\n",
        "    if cuda: # CUDA 사용 여부\n",
        "        tensor = tensor.cuda() # GPU로 이동\n",
        "    return tensor\n",
        "\n",
        "def scalar(data):\n",
        "    if isinstance(data, np.ndarray):\n",
        "        return data.reshape(1)[0]\n",
        "    if isinstance(data, torch.Tensor):\n",
        "        return data.item()"
      ],
      "metadata": {
        "id": "5N0IL4KB6yPn"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "하이퍼 파라미터 세팅"
      ],
      "metadata": {
        "id": "oraGBrVX7mEM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "epochs=14\n",
        "learning_rate = 1e-3\n",
        "lr_decay = 0.1\n",
        "weight_decay = 0.0005\n",
        "use_drop = False   # use dropout in RoIHead\n",
        "\n",
        "rpn_sigma = 3.     # sigma for l1_smooth_loss (RPN loss)\n",
        "roi_sigma = 1.     # sigma for l1_smooth_loss (ROI loss)\n",
        "\n",
        "data_dir = '../../dataset'   # 데이터 경로\n",
        "train_load_path = None  # train시 checkpoint 경로\n",
        "inf_load_path = './checkpoints/faster_rcnn_scratch_checkpoints.pth' # inference시 체크포인트 경로\n"
      ],
      "metadata": {
        "id": "KBHG7l1o6yVZ"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Dataset 만들기**\n",
        "\n",
        "1. custom data 불러오기"
      ],
      "metadata": {
        "id": "8i4oAnOP7sBX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# TrainDataset\n",
        "class TrainCustom(Dataset):\n",
        "    def __init__(self, annotation, data_dir, transforms = False):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            annotation: annotation 파일 위치\n",
        "            data_dir: data가 존재하는 폴더 경로\n",
        "            transforms : transform or not\n",
        "        \"\"\"\n",
        "\n",
        "        super().__init__()\n",
        "        self.data_dir = data_dir\n",
        "        # coco annotation 불러오기 (coco API)\n",
        "        self.coco = COCO(annotation)\n",
        "        self.transforms = transforms\n",
        "\n",
        "    def __getitem__(self, index: int):\n",
        "\n",
        "        # 이미지 아이디 가져오기\n",
        "        image_id = self.coco.getImgIds(imgIds=index)\n",
        "\n",
        "        # 이미지 정보 가져오기\n",
        "        image_info = self.coco.loadImgs(image_id)[0]\n",
        "\n",
        "        # 이미지 로드\n",
        "        image = cv2.imread(os.path.join(self.data_dir, image_info['file_name']))\n",
        "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB).astype(np.float32)\n",
        "        image /= 255.0\n",
        "\n",
        "        # 어노테이션 파일 로드\n",
        "        ann_ids = self.coco.getAnnIds(imgIds=image_info['id'])\n",
        "        anns = self.coco.loadAnns(ann_ids)\n",
        "\n",
        "        # 박스 가져오기\n",
        "        boxes = np.array([x['bbox'] for x in anns])\n",
        "\n",
        "        # boxes (x_min, y_min, x_max, y_max)\n",
        "        boxes[:, 2] = boxes[:, 0] + boxes[:, 2]\n",
        "        boxes[:, 3] = boxes[:, 1] + boxes[:, 3]\n",
        "\n",
        "        # 레이블 가져오기\n",
        "        labels = np.array([x['category_id'] for x in anns])\n",
        "        labels = torch.as_tensor(labels, dtype=torch.int64)\n",
        "\n",
        "        # transform 함수 정의\n",
        "        if self.transforms :\n",
        "            scale = 1.0  # resize scale\n",
        "            H, W, _ = image.shape\n",
        "            resize_H = int(scale * H)\n",
        "            resize_W = int(scale * W)\n",
        "            transforms = get_train_transform(resize_H, resize_W)\n",
        "        else :\n",
        "            scale = 1.0\n",
        "            transforms = no_transform()\n",
        "\n",
        "        # transform\n",
        "        sample = {\n",
        "            'image': image,\n",
        "            'bboxes': boxes,\n",
        "            'labels': labels\n",
        "        }\n",
        "        sample = transforms(**sample)\n",
        "        image = sample['image']\n",
        "        bboxes = torch.tensor(sample['bboxes'], dtype=torch.float32)\n",
        "        boxes = torch.tensor(sample['bboxes'], dtype=torch.float32)\n",
        "\n",
        "        # bboxes (x_min, y_min, x_max, y_max) -> boxes (y_min, x_min, y_max, x_max)\n",
        "        boxes[:, 0] = bboxes[:, 1]\n",
        "        boxes[:, 1] = bboxes[:, 0]\n",
        "        boxes[:, 2] = bboxes[:, 3]\n",
        "        boxes[:, 3] = bboxes[:, 2]\n",
        "\n",
        "        return image, boxes, labels, scale\n",
        "\n",
        "    def __len__(self) -> int:\n",
        "        return len(self.coco.getImgIds())\n",
        "\n",
        "# Test Datset\n",
        "class TestCustom(Dataset):\n",
        "    def __init__(self, annotation, data_dir):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            annotation: annotation 파일 위치\n",
        "            data_dir: data가 존재하는 폴더 경로\n",
        "        \"\"\"\n",
        "\n",
        "        super().__init__()\n",
        "        self.data_dir = data_dir\n",
        "        # coco annotation 불러오기 (coco API)\n",
        "        self.coco = COCO(annotation)\n",
        "\n",
        "    def __getitem__(self, index: int):\n",
        "\n",
        "        # 이미지 아이디 가져오기\n",
        "        image_id = self.coco.getImgIds(imgIds=index)\n",
        "\n",
        "        # 이미지 정보 가져오기\n",
        "        image_info = self.coco.loadImgs(image_id)[0]\n",
        "\n",
        "        # 이미지 로드\n",
        "        image = cv2.imread(os.path.join(self.data_dir, image_info['file_name']))\n",
        "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB).astype(np.float32)\n",
        "        image /= 255.0\n",
        "        image = torch.tensor(image, dtype = torch.float).permute(2,0,1)\n",
        "\n",
        "        return image, image.shape[1:]\n",
        "\n",
        "    def __len__(self) -> int:\n",
        "        return len(self.coco.getImgIds())\n"
      ],
      "metadata": {
        "id": "HAfR-B5F7prs"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. Transform"
      ],
      "metadata": {
        "id": "ocpJDfUs74b1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Train dataset transform\n",
        "def get_train_transform(h, w):\n",
        "    return A.Compose([\n",
        "        A.Resize(height = h, width = w),\n",
        "        A.Flip(p=0.5),\n",
        "        ToTensorV2(p=1.0)\n",
        "    ], bbox_params={'format': 'pascal_voc', 'label_fields': ['labels']})\n",
        "\n",
        "# No transform\n",
        "def no_transform():\n",
        "    return A.Compose([\n",
        "        ToTensorV2(p=1.0) # format for pytorch tensor\n",
        "    ], bbox_params={'format': 'pascal_voc', 'label_fields': ['labels']})"
      ],
      "metadata": {
        "id": "ze-AfMIU74vn"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **RPN (Region Proposal Network) 정의**\n",
        "\n",
        "1. Anchor box 생성 (generate_anchor_base)"
      ],
      "metadata": {
        "id": "yXwhZFjf79ew"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_anchor_base(base_size=16, ratios=[0.5, 1, 2], anchor_scales=[8, 16, 32]): # 한 픽셀당 몇 개의 엥커박스 생성?\n",
        "    \"\"\"\n",
        "    Args:\n",
        "        ratios: 비율\n",
        "        anchor_scales: 스케일\n",
        "    Returns: basic anchor boxes, shape=(R, 4)\n",
        "        R: len(ratio) * len(anchor_scales) = anchor 개수 = 9\n",
        "        4: anchor box 좌표 값\n",
        "    \"\"\"\n",
        "\n",
        "    py = base_size / 2. # center y\n",
        "    px = base_size / 2. # center x\n",
        "\n",
        "    anchor_base = np.zeros((len(ratios) * len(anchor_scales), 4), dtype=np.float32) # anchor_box\n",
        "\n",
        "    for i in six.moves.range(len(ratios)):\n",
        "        for j in six.moves.range(len(anchor_scales)):\n",
        "            h = base_size * anchor_scales[j] * np.sqrt(ratios[i])\n",
        "            w = base_size * anchor_scales[j] * np.sqrt(1. / ratios[i])\n",
        "\n",
        "            index = i * len(anchor_scales) + j\n",
        "            # offset of anchor box\n",
        "            anchor_base[index, 0] = py - h / 2. # y_min\n",
        "            anchor_base[index, 1] = px - w / 2. # x_min\n",
        "            anchor_base[index, 2] = py + h / 2. # y_max\n",
        "            anchor_base[index, 3] = px + w / 2. # x_max\n",
        "\n",
        "    return anchor_base # (9,4)"
      ],
      "metadata": {
        "id": "fpkq0ZZ87__T"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2. Proposal 생성 (ProposalCreator)\n",
        "\n",
        "RPN에서 구한 rpn_loc와 anchor을 통해서 Region of Interest(RoI)를 생성\n",
        "RoI 개수를 줄이기 위해서 미리 정해둔 크기(min_size)에 맞는 roi들 중 NMS를 통해 최종 RoI 반환 (train 시 2000개)"
      ],
      "metadata": {
        "id": "RzzX7Tac8FLa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class ProposalCreator:\n",
        "    def __init__(self, parent_model,\n",
        "                 nms_thresh=0.7, # nms threshold\n",
        "                 n_train_pre_nms=12000, # train시 nms 전 roi 개수\n",
        "                 n_train_post_nms=2000, # train시 nms 후 roi 개수\n",
        "                 n_test_pre_nms=6000,   # test시 nms 전 roi 개수\n",
        "                 n_test_post_nms=300,   # test시 nms 후 roi 개수\n",
        "                 min_size=16\n",
        "                 ):\n",
        "        self.parent_model = parent_model # 해당 모델이 train중인지 test중인지 나타냄\n",
        "        self.nms_thresh = nms_thresh\n",
        "        self.n_train_pre_nms = n_train_pre_nms\n",
        "        self.n_train_post_nms = n_train_post_nms\n",
        "        self.n_test_pre_nms = n_test_pre_nms\n",
        "        self.n_test_post_nms = n_test_post_nms\n",
        "        self.min_size = min_size\n",
        "\n",
        "    def __call__(self, loc, score, anchor, img_size, scale=1.):\n",
        "        if self.parent_model.training: # train중일 때\n",
        "            n_pre_nms = self.n_train_pre_nms\n",
        "            n_post_nms = self.n_train_post_nms\n",
        "        else: # test중일 때\n",
        "            n_pre_nms = self.n_test_pre_nms\n",
        "            n_post_nms = self.n_test_post_nms\n",
        "\n",
        "        roi = loc2bbox(anchor, loc) # anchor의 좌표값과 predicted bounding bounding box offset(y,x,h,w)를 통해 bounding box 좌표값(y_min, x_min, y_max, x_max) 생성\n",
        "\n",
        "        # Clip predicted boxes to image.\n",
        "        roi[:, slice(0, 4, 2)] = np.clip(roi[:, slice(0, 4, 2)], 0, img_size[0])\n",
        "        roi[:, slice(1, 4, 2)] = np.clip(roi[:, slice(1, 4, 2)], 0, img_size[1])\n",
        "\n",
        "        # min_size 보다 작은 box들은 제거\n",
        "        min_size = self.min_size * scale\n",
        "        hs = roi[:, 2] - roi[:, 0]\n",
        "        ws = roi[:, 3] - roi[:, 1]\n",
        "        keep = np.where((hs >= min_size) & (ws >= min_size))[0]\n",
        "        roi = roi[keep, :]\n",
        "        score = score[keep]\n",
        "\n",
        "        # Sort all (proposal, score) pairs by score from highest to lowest.\n",
        "        # Take top pre_nms_topN\n",
        "        order = score.ravel().argsort()[::-1]\n",
        "        if n_pre_nms > 0:\n",
        "            order = order[:n_pre_nms]\n",
        "        roi = roi[order, :]\n",
        "        score = score[order]\n",
        "\n",
        "        # nms 적용\n",
        "        keep = nms(\n",
        "            torch.from_numpy(roi).cuda(),\n",
        "            torch.from_numpy(score).cuda(),\n",
        "            self.nms_thresh)\n",
        "        if n_post_nms > 0:\n",
        "            keep = keep[:n_post_nms]\n",
        "        roi = roi[keep.cpu().numpy()] # 최종적으로 2000개만 사용\n",
        "\n",
        "        return roi"
      ],
      "metadata": {
        "id": "3rzbfHYe8Fov"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3. region proposal network\n",
        "\n",
        "VGG16 통과한 feature map으로부터 region proposal들 생성"
      ],
      "metadata": {
        "id": "jBA_em_x8Ksp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class RegionProposalNetwork(nn.Module):\n",
        "    def __init__(self, in_channels=512, mid_channels=512, ratios=[0.5, 1, 2],\n",
        "                 anchor_scales=[8, 16, 32], feat_stride=16, proposal_creator_params=dict(),):\n",
        "\n",
        "        super(RegionProposalNetwork, self).__init__()\n",
        "\n",
        "        self.anchor_base = generate_anchor_base(anchor_scales=anchor_scales, ratios=ratios) # 9개의 anchorbox 생성\n",
        "        self.feat_stride = feat_stride\n",
        "        self.proposal_layer = ProposalCreator(self, **proposal_creator_params) # proposal_creator_params : 해당 네트워크가 training인지 testing인지 알려준다.\n",
        "        n_anchor = self.anchor_base.shape[0] # anchor 개수\n",
        "        self.conv1 = nn.Conv2d(in_channels, mid_channels, 3, 1, 1)\n",
        "        self.score = nn.Conv2d(mid_channels, n_anchor * 2, 1, 1, 0)  # 9*2\n",
        "        self.loc = nn.Conv2d(mid_channels, n_anchor * 4, 1, 1, 0)   # 9*4\n",
        "        normal_init(self.conv1, 0, 0.01) # weight initalizer\n",
        "        normal_init(self.score, 0, 0.01) # weight initalizer\n",
        "        normal_init(self.loc, 0, 0.01)   # weight initalizer\n",
        "\n",
        "    def forward(self, x, img_size, scale=1.):\n",
        "        # x(feature map)\n",
        "        n, _, hh, ww = x.shape\n",
        "\n",
        "        # 전체 (h*w*9)개 anchor의 좌표값 # anchor_base:(9, 4)\n",
        "        anchor = _enumerate_shifted_anchor(np.array(self.anchor_base), self.feat_stride, hh, ww)\n",
        "        n_anchor = anchor.shape[0] // (hh * ww) # anchor 개수\n",
        "\n",
        "        middle = F.relu(self.conv1(x))\n",
        "\n",
        "        # predicted bounding box offset\n",
        "        rpn_locs = self.loc(middle)\n",
        "        rpn_locs = rpn_locs.permute(0, 2, 3, 1).contiguous().view(n, -1, 4)\n",
        "\n",
        "        # predicted scores for anchor (foreground or background)\n",
        "        rpn_scores = self.score(middle)\n",
        "        rpn_scores = rpn_scores.permute(0, 2, 3, 1).contiguous()\n",
        "\n",
        "        # scores for foreground\n",
        "        rpn_softmax_scores = F.softmax(rpn_scores.view(n, hh, ww, n_anchor, 2), dim=4)\n",
        "        rpn_fg_scores = rpn_softmax_scores[:, :, :, :, 1].contiguous()\n",
        "        rpn_fg_scores = rpn_fg_scores.view(n, -1)\n",
        "\n",
        "        rpn_scores = rpn_scores.view(n, -1, 2)\n",
        "\n",
        "        # proposal생성 (ProposalCreator)\n",
        "        rois = list()        # proposal의 좌표값이 있는 bounding box array\n",
        "        roi_indices = list() # roi에 해당하는 image 인덱스\n",
        "        for i in range(n):\n",
        "            roi = self.proposal_layer(rpn_locs[i].cpu().data.numpy(),rpn_fg_scores[i].cpu().data.numpy(),anchor, img_size,scale=scale)\n",
        "            batch_index = i * np.ones((len(roi),), dtype=np.int32)\n",
        "            rois.append(roi)\n",
        "            roi_indices.append(batch_index)\n",
        "        rois = np.concatenate(rois, axis=0)\n",
        "        roi_indices = np.concatenate(roi_indices, axis=0)\n",
        "\n",
        "        return rpn_locs, rpn_scores, rois, roi_indices, anchor # 2000개 RoI 리턴\n",
        "\n",
        "\n",
        "def _enumerate_shifted_anchor(anchor_base, feat_stride, height, width):\n",
        "    # anchor_base는 하나의 pixel에 9개 종류의 anchor box를 나타냄\n",
        "    # 이것을 enumerate시켜 전체 이미지의 pixel에 각각 9개의 anchor box를 가지게 함\n",
        "    # 32x32 feature map에서는 32x32x9=9216개 anchor box가짐\n",
        "\n",
        "    shift_y = np.arange(0, height * feat_stride, feat_stride)\n",
        "    shift_x = np.arange(0, width * feat_stride, feat_stride)\n",
        "    shift_x, shift_y = np.meshgrid(shift_x, shift_y)\n",
        "    shift = np.stack((shift_y.ravel(), shift_x.ravel(),\n",
        "                      shift_y.ravel(), shift_x.ravel()), axis=1)\n",
        "\n",
        "    A = anchor_base.shape[0]\n",
        "    K = shift.shape[0]\n",
        "    anchor = anchor_base.reshape((1, A, 4)) + \\\n",
        "             shift.reshape((1, K, 4)).transpose((1, 0, 2))\n",
        "    anchor = anchor.reshape((K * A, 4)).astype(np.float32)\n",
        "    return anchor # (9216, 4)"
      ],
      "metadata": {
        "id": "Z88t_mFc8L5O"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Feature extractor(VGG) 정의"
      ],
      "metadata": {
        "id": "nEX1Chp68PMu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def decom_vgg16():\n",
        "    # the 30th layer of features is relu of conv5_3\n",
        "    model = vgg16(pretrained=True)\n",
        "\n",
        "    features = list(model.features)[:30]\n",
        "    classifier = model.classifier\n",
        "\n",
        "    classifier = list(classifier)\n",
        "    del classifier[6]\n",
        "    if not use_drop:\n",
        "        del classifier[5]\n",
        "        del classifier[2]\n",
        "    classifier = nn.Sequential(*classifier)\n",
        "\n",
        "    # freeze top4 conv\n",
        "    for layer in features[:10]:\n",
        "        for p in layer.parameters():\n",
        "            p.requires_grad = False\n",
        "\n",
        "    return nn.Sequential(*features), classifier"
      ],
      "metadata": {
        "id": "ZIhDWpAm8RNI"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Faster R-CNN head 정의\n",
        "\n",
        "RoI pool 후에 classifier, regressor 통과 -> 클래스, 정확한 박스 예측"
      ],
      "metadata": {
        "id": "l_Q_FO3c8UE1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class VGG16RoIHead(nn.Module):\n",
        "    \"\"\"\n",
        "    Faster R-CNN head\n",
        "    RoI pool 후에 classifier, regressor 통과\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, n_class, roi_size, spatial_scale, classifier):\n",
        "        super(VGG16RoIHead, self).__init__()\n",
        "\n",
        "        self.classifier = classifier\n",
        "        self.cls_loc = nn.Linear(4096, n_class * 4) # bounding box regressor\n",
        "        self.score = nn.Linear(4096, n_class) # Classifier\n",
        "\n",
        "        normal_init(self.cls_loc, 0, 0.001)  # weight initialize\n",
        "        normal_init(self.score, 0, 0.01)     # weight initialize\n",
        "\n",
        "        self.n_class = n_class # 배경 포함한 class 수\n",
        "        self.roi_size = roi_size # RoI-pooling 후 feature map의  높이, 너비 -> 타겟 사이\n",
        "        self.spatial_scale = spatial_scale # roi resize scale\n",
        "        self.roi = RoIPool( (self.roi_size, self.roi_size),self.spatial_scale)\n",
        "\n",
        "    def forward(self, x, rois, roi_indices):\n",
        "        # in case roi_indices is  ndarray\n",
        "        roi_indices = totensor(roi_indices).float()\n",
        "        rois = totensor(rois).float()\n",
        "        indices_and_rois = torch.cat([roi_indices[:, None], rois], dim=1)\n",
        "        # NOTE: important: yx->xy\n",
        "        xy_indices_and_rois = indices_and_rois[:, [0, 2, 1, 4, 3]]\n",
        "        indices_and_rois =  xy_indices_and_rois.contiguous()\n",
        "\n",
        "        # 각 이미지 x([1, 512, 32, 32])에 맞게 roi pooling\n",
        "        pool = self.roi(x, indices_and_rois)  # torch.Size([128, 512, 7,7])\n",
        "        # flatten\n",
        "        pool = pool.view(pool.size(0), -1) # torch.Size([128, 25088])\n",
        "        # fully connected\n",
        "        fc7 = self.classifier(pool) # # torch.Size([128, 4096])\n",
        "        # regression\n",
        "        roi_cls_locs = self.cls_loc(fc7) # torch.Size([128, 48])\n",
        "        # softmax\n",
        "        roi_scores = self.score(fc7) # torch.Size([128, 12])\n",
        "\n",
        "\n",
        "        return roi_cls_locs, roi_scores"
      ],
      "metadata": {
        "id": "sZoVy20J8Vde"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Faster R-CNN 정의\n",
        "\n",
        "\n",
        "\n",
        "*   Feature Extraction: image로부터 feature map 생성\n",
        "*   Region Proposal Networks: Region of Interest 생성\n",
        "*   Localization and Classification Head: RoI에 해당하는 feature map을 최종 detect\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "E_A9KDDB8YN4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def nograd(f):\n",
        "    def new_f(*args, **kwargs):\n",
        "        with torch.no_grad():\n",
        "            return f(*args, **kwargs)\n",
        "    return new_f\n",
        "\n",
        "class FasterRCNN(nn.Module):\n",
        "    def __init__(self, extractor, rpn, head,\n",
        "                loc_normalize_mean = (0., 0., 0., 0.),\n",
        "                loc_normalize_std = (0.1, 0.1, 0.2, 0.2)):\n",
        "        super(FasterRCNN, self).__init__()\n",
        "        self.extractor = extractor  # extractor : vgg\n",
        "        self.rpn = rpn              # rpn : region proposal network\n",
        "        self.head = head            # head : RoiHead\n",
        "\n",
        "        # mean and std\n",
        "        self.loc_normalize_mean = loc_normalize_mean\n",
        "        self.loc_normalize_std = loc_normalize_std\n",
        "        self.use_preset()\n",
        "\n",
        "    @property\n",
        "    def n_class(self): # 최종 class 개수 (배경 포함)\n",
        "        return self.head.n_class\n",
        "\n",
        "    # predict 시 사용하는 forward\n",
        "    # train 시 FasterRCNNTrainer을 사용하여 FasterRcnn에 있는 extractor, rpn, head를 모듈별로 불러와서 forward\n",
        "    def forward(self, x, scale=1.):\n",
        "        img_size = x.shape[2:]\n",
        "\n",
        "        h = self.extractor(x) # extractor 통과\n",
        "        rpn_locs, rpn_scores, rois, roi_indices, anchor = self.rpn(h, img_size, scale) # rpn 통과\n",
        "        roi_cls_locs, roi_scores = self.head(h, rois, roi_indices) # head 통과\n",
        "        return roi_cls_locs, roi_scores, rois, roi_indices\n",
        "\n",
        "    def use_preset(self): # prediction 과정 쓰이는 threshold 정의\n",
        "        self.nms_thresh = 0.3\n",
        "        self.score_thresh = 0.05\n",
        "\n",
        "    def _suppress(self, raw_cls_bbox, raw_prob):\n",
        "        bbox = list()\n",
        "        label = list()\n",
        "        score = list()\n",
        "\n",
        "        # skip cls_id = 0 because it is the background class\n",
        "        for l in range(1, self.n_class):\n",
        "            cls_bbox_l = raw_cls_bbox.reshape((-1, self.n_class, 4))[:, l, :]\n",
        "            prob_l = raw_prob[:, l]\n",
        "            mask = prob_l > self.score_thresh\n",
        "            cls_bbox_l = cls_bbox_l[mask]\n",
        "            prob_l = prob_l[mask]\n",
        "            keep = nms(cls_bbox_l, prob_l,self.nms_thresh)\n",
        "            bbox.append(cls_bbox_l[keep].cpu().numpy())\n",
        "            # The labels are in [0, self.n_class - 2].\n",
        "            label.append((l - 1) * np.ones((len(keep),)))\n",
        "            score.append(prob_l[keep].cpu().numpy())\n",
        "\n",
        "        bbox = np.concatenate(bbox, axis=0).astype(np.float32)\n",
        "        label = np.concatenate(label, axis=0).astype(np.int32)\n",
        "        score = np.concatenate(score, axis=0).astype(np.float32)\n",
        "        return bbox, label, score\n",
        "\n",
        "    @nograd\n",
        "    def predict(self, imgs,sizes=None):\n",
        "        \"\"\"\n",
        "        이미지에서 객체 검출\n",
        "        Input : images\n",
        "        Output : bboxes, labels, scores\n",
        "        \"\"\"\n",
        "        self.eval()\n",
        "        prepared_imgs = imgs\n",
        "\n",
        "        bboxes = list()\n",
        "        labels = list()\n",
        "        scores = list()\n",
        "        for img, size in zip(prepared_imgs, sizes):\n",
        "            img = totensor(img[None]).float()\n",
        "            scale = img.shape[3] / size[1]\n",
        "            roi_cls_loc, roi_scores, rois, _ = self(img, scale=scale) # self = FasterRCNN\n",
        "            # We are assuming that batch size is 1.\n",
        "            roi_score = roi_scores.data\n",
        "            roi_cls_loc = roi_cls_loc.data\n",
        "            roi = totensor(rois) / scale\n",
        "\n",
        "            # Convert predictions to bounding boxes in image coordinates.\n",
        "            # Bounding boxes are scaled to the scale of the input images.\n",
        "            mean = torch.Tensor(self.loc_normalize_mean).cuda(). repeat(self.n_class)[None]\n",
        "            std = torch.Tensor(self.loc_normalize_std).cuda(). repeat(self.n_class)[None]\n",
        "\n",
        "            roi_cls_loc = (roi_cls_loc * std + mean)\n",
        "            roi_cls_loc = roi_cls_loc.view(-1, self.n_class, 4)\n",
        "            roi = roi.view(-1, 1, 4).expand_as(roi_cls_loc)\n",
        "            cls_bbox = loc2bbox(tonumpy(roi).reshape((-1, 4)),tonumpy(roi_cls_loc).reshape((-1, 4)))\n",
        "            cls_bbox = totensor(cls_bbox)\n",
        "            cls_bbox = cls_bbox.view(-1, self.n_class * 4)\n",
        "            # clip bounding box\n",
        "            cls_bbox[:, 0::2] = (cls_bbox[:, 0::2]).clamp(min=0, max=size[0])\n",
        "            cls_bbox[:, 1::2] = (cls_bbox[:, 1::2]).clamp(min=0, max=size[1])\n",
        "\n",
        "            prob = (F.softmax(totensor(roi_score), dim=1))\n",
        "\n",
        "            bbox, label, score = self._suppress(cls_bbox, prob)\n",
        "            bboxes.append(bbox)\n",
        "            labels.append(label)\n",
        "            scores.append(score)\n",
        "\n",
        "        self.use_preset()\n",
        "        self.train()\n",
        "        return bboxes, labels, scores\n",
        "\n",
        "    def get_optimizer(self):\n",
        "        '''\n",
        "        Optimizer 선언\n",
        "        '''\n",
        "        lr = learning_rate\n",
        "        params = []\n",
        "        for key, value in dict(self.named_parameters()).items():\n",
        "            if value.requires_grad:\n",
        "                if 'bias' in key:\n",
        "                    params += [{'params': [value], 'lr': lr * 2, 'weight_decay': 0}]\n",
        "                else:\n",
        "                    params += [{'params': [value], 'lr': lr, 'weight_decay': weight_decay}]\n",
        "        self.optimizer = torch.optim.SGD(params, momentum=0.9)\n",
        "        return self.optimizer\n",
        "\n",
        "    def scale_lr(self, decay=0.1):\n",
        "        for param_group in self.optimizer.param_groups:\n",
        "            param_group['lr'] *= decay\n",
        "        return self.optimizer"
      ],
      "metadata": {
        "id": "BRW7HrTi8Zl8"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Faster R-CNN 생성\n",
        "\n",
        "Extractor(VGG) + RPN + Head 합치기"
      ],
      "metadata": {
        "id": "XzHoq6Rp8dcR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class FasterRCNNVGG16(FasterRCNN):\n",
        "\n",
        "    feat_stride = 16  # downsample 16x for output of conv5 in vgg16\n",
        "\n",
        "    def __init__(self, n_fg_class=10, ratios=[0.5, 1, 2], anchor_scales=[8, 16, 32] ): # n_fg_class : 배경포함 하지 않은 class 개수\n",
        "        extractor, classifier = decom_vgg16()\n",
        "\n",
        "        rpn = RegionProposalNetwork(\n",
        "            512, 512,\n",
        "            ratios=ratios,\n",
        "            anchor_scales=anchor_scales,\n",
        "            feat_stride=self.feat_stride,\n",
        "        )\n",
        "\n",
        "        head = VGG16RoIHead(\n",
        "            n_class=n_fg_class + 1,\n",
        "            roi_size=7,\n",
        "            spatial_scale=(1. / self.feat_stride),\n",
        "            classifier=classifier\n",
        "        )\n",
        "        super(FasterRCNNVGG16, self).__init__(\n",
        "            extractor,\n",
        "            rpn,\n",
        "            head,\n",
        "        )"
      ],
      "metadata": {
        "id": "f5G2OLuw8gOT"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Trainer\n",
        "\n",
        "0. util 함수 정의 -> train set 정의"
      ],
      "metadata": {
        "id": "yQy6Z0Ex8hXE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def bbox_iou(bbox_a, bbox_b):\n",
        "    if bbox_a.shape[1] != 4 or bbox_b.shape[1] != 4:\n",
        "        raise IndexError\n",
        "\n",
        "    #bbox_a 1개와 bbox_b k개를 비교해야하므로 None을 이용해서 차원을 늘려서 연산한다.\n",
        "    # top left\n",
        "    tl = np.maximum(bbox_a[:, None, :2], bbox_b[:, :2])\n",
        "    # bottom right\n",
        "    br = np.minimum(bbox_a[:, None, 2:], bbox_b[:, 2:])\n",
        "\n",
        "    area_i = np.prod(br - tl, axis=2) * (tl < br).all(axis=2)\n",
        "    area_a = np.prod(bbox_a[:, 2:] - bbox_a[:, :2], axis=1)\n",
        "    area_b = np.prod(bbox_b[:, 2:] - bbox_b[:, :2], axis=1)\n",
        "    return area_i / (area_a[:, None] + area_b - area_i)\n"
      ],
      "metadata": {
        "id": "0FbpLh8b8jzO"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1. Anchor Target Creator\n",
        "\n",
        "- Anchor box에 해당하는 ground truth bounding box match\n",
        "- Region Proposal Network loss 구할 때 ground truth로 사용"
      ],
      "metadata": {
        "id": "zV8MxgBG8mgO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class AnchorTargetCreator(object): # 어떤 엥커 박스가 negative, positive인지 확인\n",
        "\n",
        "    def __init__(self,\n",
        "                 n_sample=256,\n",
        "                 pos_iou_thresh=0.7, neg_iou_thresh=0.3,\n",
        "                 pos_ratio=0.5):\n",
        "        self.n_sample = n_sample\n",
        "        self.pos_iou_thresh = pos_iou_thresh\n",
        "        self.neg_iou_thresh = neg_iou_thresh\n",
        "        self.pos_ratio = pos_ratio\n",
        "\n",
        "    def __call__(self, bbox, anchor, img_size):\n",
        "\n",
        "        img_H, img_W = img_size\n",
        "\n",
        "        n_anchor = len(anchor) # 9216\n",
        "        inside_index = get_inside_index(anchor, img_H, img_W) # (2272,)\n",
        "        anchor = anchor[inside_index] # (2272, 4)\n",
        "        argmax_ious, label = self._create_label(\n",
        "            inside_index, anchor, bbox)\n",
        "\n",
        "        # compute bounding box regression targets\n",
        "        loc = bbox2loc(anchor, bbox[argmax_ious]) # (2272, 4)\n",
        "\n",
        "        # map up to original set of anchors\n",
        "        label = unmap(label, n_anchor, inside_index, fill=-1) # (9216,)\n",
        "        loc = unmap(loc, n_anchor, inside_index, fill=0) # (9216, 4)\n",
        "\n",
        "        return loc, label\n",
        "\n",
        "    def _create_label(self, inside_index, anchor, bbox):\n",
        "        # label) 1 :positive, 0 : negative, -1 : dont care\n",
        "        label = np.empty((len(inside_index),), dtype=np.int32)\n",
        "        label.fill(-1)\n",
        "\n",
        "        argmax_ious, max_ious, gt_argmax_ious = self._calc_ious(anchor, bbox, inside_index)\n",
        "\n",
        "        label[max_ious < self.neg_iou_thresh] = 0 # 0.3\n",
        "\n",
        "        # 가장 iou가 큰 것은 positive label\n",
        "        label[gt_argmax_ious] = 1\n",
        "\n",
        "        # positive label\n",
        "        label[max_ious >= self.pos_iou_thresh] = 1 # 0.7\n",
        "\n",
        "        # subsample positive labels if we have too many\n",
        "        n_pos = int(self.pos_ratio * self.n_sample)\n",
        "        pos_index = np.where(label == 1)[0]\n",
        "        if len(pos_index) > n_pos:\n",
        "            disable_index = np.random.choice(\n",
        "                pos_index, size=(len(pos_index) - n_pos), replace=False)\n",
        "            label[disable_index] = -1\n",
        "\n",
        "        # subsample negative labels if we have too many\n",
        "        n_neg = self.n_sample - np.sum(label == 1)\n",
        "        neg_index = np.where(label == 0)[0]\n",
        "        if len(neg_index) > n_neg:\n",
        "            disable_index = np.random.choice(\n",
        "                neg_index, size=(len(neg_index) - n_neg), replace=False)\n",
        "            label[disable_index] = -1\n",
        "\n",
        "        return argmax_ious, label\n",
        "\n",
        "    def _calc_ious(self, anchor, bbox, inside_index):\n",
        "        # ious between the anchors and the gt boxes\n",
        "        ious = bbox_iou(anchor, bbox)\n",
        "        argmax_ious = ious.argmax(axis=1)\n",
        "        max_ious = ious[np.arange(len(inside_index)), argmax_ious]\n",
        "        gt_argmax_ious = ious.argmax(axis=0)\n",
        "        gt_max_ious = ious[gt_argmax_ious, np.arange(ious.shape[1])]\n",
        "        gt_argmax_ious = np.where(ious == gt_max_ious)[0]\n",
        "\n",
        "        return argmax_ious, max_ious, gt_argmax_ious\n"
      ],
      "metadata": {
        "id": "V5g7qd6r8oCS"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. positive, negative sampling"
      ],
      "metadata": {
        "id": "NOqUwmc68qdd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class ProposalTargetCreator:\n",
        "    def __init__(self,\n",
        "                 n_sample=128,\n",
        "                 pos_ratio=0.25, pos_iou_thresh=0.5,\n",
        "                 neg_iou_thresh_hi=0.5, neg_iou_thresh_lo=0.0\n",
        "                 ):\n",
        "        self.n_sample = n_sample\n",
        "        self.pos_ratio = pos_ratio\n",
        "        self.pos_iou_thresh = pos_iou_thresh # positive iou threshold\n",
        "        self.neg_iou_thresh_hi = neg_iou_thresh_hi # negitave iou threshold = (neg_iou_thresh_hi ~ neg_iou_thresh_lo)\n",
        "        self.neg_iou_thresh_lo = neg_iou_thresh_lo\n",
        "\n",
        "    def __call__(self, roi, bbox, label,\n",
        "                 loc_normalize_mean=(0., 0., 0., 0.),\n",
        "                 loc_normalize_std=(0.1, 0.1, 0.2, 0.2)):\n",
        "        n_bbox, _ = bbox.shape\n",
        "\n",
        "        roi = np.concatenate((roi, bbox), axis=0)\n",
        "\n",
        "        pos_roi_per_image = np.round(self.n_sample * self.pos_ratio) # positive image 갯수 = 32\n",
        "        iou = bbox_iou(roi, bbox) # RoI와 bounding box IoU\n",
        "        gt_assignment = iou.argmax(axis=1)\n",
        "        max_iou = iou.max(axis=1)\n",
        "        gt_roi_label = label[gt_assignment] + 1 # class label [0, n_fg_class - 1] -> [1, n_fg_class].\n",
        "\n",
        "        # positive sample 선택 (>= pos_iou_thresh IoU)\n",
        "        pos_index = np.where(max_iou >= self.pos_iou_thresh)[0]\n",
        "        pos_roi_per_this_image = int(min(pos_roi_per_image, pos_index.size))\n",
        "        if pos_index.size > 0:\n",
        "            pos_index = np.random.choice(\n",
        "                pos_index, size=pos_roi_per_this_image, replace=False)\n",
        "\n",
        "        # Negative sample 선택 [neg_iou_thresh_lo, neg_iou_thresh_hi)\n",
        "        neg_index = np.where((max_iou < self.neg_iou_thresh_hi) &\n",
        "                             (max_iou >= self.neg_iou_thresh_lo))[0]\n",
        "        neg_roi_per_this_image = self.n_sample - pos_roi_per_this_image\n",
        "        neg_roi_per_this_image = int(min(neg_roi_per_this_image,\n",
        "                                         neg_index.size))\n",
        "        if neg_index.size > 0:\n",
        "            neg_index = np.random.choice(\n",
        "                neg_index, size=neg_roi_per_this_image, replace=False)\n",
        "\n",
        "        # The indices that we're selecting (both positive and negative).\n",
        "        keep_index = np.append(pos_index, neg_index)\n",
        "        gt_roi_label = gt_roi_label[keep_index]\n",
        "        gt_roi_label[pos_roi_per_this_image:] = 0  # negative sample의 label = 0\n",
        "        sample_roi = roi[keep_index] # (128, 4)\n",
        "\n",
        "        # sample roi와 gt_bbox를 이용해 bbox regression에서 regression해야할 ground truth loc값(t_x, t_y, t_w, t_h) 계산\n",
        "        gt_roi_loc = bbox2loc(sample_roi, bbox[gt_assignment[keep_index]]) # (128, 4)\n",
        "        gt_roi_loc = ((gt_roi_loc - np.array(loc_normalize_mean, np.float32)) / np.array(loc_normalize_std, np.float32))\n",
        "\n",
        "        return sample_roi, gt_roi_loc, gt_roi_label"
      ],
      "metadata": {
        "id": "d8Mo9z-b8rrj"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "3. Trainer 정의"
      ],
      "metadata": {
        "id": "zmCwCXUX8xmC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "LossTuple = namedtuple('LossTuple', ['rpn_loc_loss', 'rpn_cls_loss',\n",
        "                                     'roi_loc_loss', 'roi_cls_loss',\n",
        "                                     'total_loss'])\n",
        "class FasterRCNNTrainer(nn.Module):\n",
        "\n",
        "    def __init__(self, faster_rcnn):\n",
        "        super(FasterRCNNTrainer, self).__init__()\n",
        "\n",
        "        self.faster_rcnn = faster_rcnn\n",
        "        self.rpn_sigma = rpn_sigma\n",
        "        self.roi_sigma = roi_sigma\n",
        "\n",
        "        # target creator create gt_bbox gt_label etc as training targets.\n",
        "        self.anchor_target_creator = AnchorTargetCreator()\n",
        "        self.proposal_target_creator = ProposalTargetCreator()\n",
        "\n",
        "        self.loc_normalize_mean = faster_rcnn.loc_normalize_mean\n",
        "        self.loc_normalize_std = faster_rcnn.loc_normalize_std\n",
        "\n",
        "        self.optimizer = self.faster_rcnn.get_optimizer()\n",
        "\n",
        "        # training 상태 보여주는 지표\n",
        "        self.rpn_cm = ConfusionMeter(2) # confusion matrix for classification\n",
        "        self.roi_cm = ConfusionMeter(11)  # confusion matrix for classification\n",
        "        self.meters = {k: AverageValueMeter() for k in LossTuple._fields}  # average loss\n",
        "\n",
        "    def forward(self, imgs, bboxes, labels, scale):\n",
        "        n = bboxes.shape[0]\n",
        "\n",
        "        if n != 1:\n",
        "            raise ValueError('Currently only batch size 1 is supported.')\n",
        "\n",
        "        _, _, H, W = imgs.shape\n",
        "        img_size = (H, W)\n",
        "\n",
        "        # VGG (features extractor)\n",
        "        features = self.faster_rcnn.extractor(imgs)\n",
        "\n",
        "        # RPN (region proposal)\n",
        "        rpn_locs, rpn_scores, rois, roi_indices, anchor = self.faster_rcnn.rpn(features, img_size, scale)\n",
        "\n",
        "        # Since batch size is one, convert variables to singular form\n",
        "        bbox = bboxes[0]\n",
        "        label = labels[0]\n",
        "        rpn_score = rpn_scores[0]\n",
        "        rpn_loc = rpn_locs[0]\n",
        "        roi = rois\n",
        "\n",
        "        \"\"\"\n",
        "        sample roi =  rpn에서 nms 거친 2000개의 roi들 중 positive/negative 비율 고려해 최종 sampling한 roi\n",
        "        \"\"\"\n",
        "        sample_roi, gt_roi_loc, gt_roi_label = self.proposal_target_creator(\n",
        "            roi,\n",
        "            tonumpy(bbox),\n",
        "            tonumpy(label),\n",
        "            self.loc_normalize_mean,\n",
        "            self.loc_normalize_std)\n",
        "\n",
        "        # NOTE it's all zero because now it only support for batch=1 now\n",
        "        # Faster R-CNN head (prediction head)\n",
        "        sample_roi_index = torch.zeros(len(sample_roi))\n",
        "        roi_cls_loc, roi_score = self.faster_rcnn.head(features,sample_roi,sample_roi_index)\n",
        "\n",
        "        # ------------------ RPN losses -------------------#\n",
        "        gt_rpn_loc, gt_rpn_label = self.anchor_target_creator(tonumpy(bbox),anchor,img_size)\n",
        "        gt_rpn_label = totensor(gt_rpn_label).long()\n",
        "        gt_rpn_loc = totensor(gt_rpn_loc)\n",
        "\n",
        "        # rpn bounding box regression loss\n",
        "        rpn_loc_loss = _fast_rcnn_loc_loss(rpn_loc,gt_rpn_loc,gt_rpn_label.data,self.rpn_sigma)\n",
        "        # rpn classification loss\n",
        "        rpn_cls_loss = F.cross_entropy(rpn_score, gt_rpn_label.cuda(), ignore_index=-1)\n",
        "\n",
        "        _gt_rpn_label = gt_rpn_label[gt_rpn_label > -1]\n",
        "        _rpn_score = tonumpy(rpn_score)[tonumpy(gt_rpn_label) > -1]\n",
        "        self.rpn_cm.add(totensor(_rpn_score, False), _gt_rpn_label.data.long())\n",
        "\n",
        "        # ------------------ ROI losses (fast rcnn loss) -------------------#\n",
        "        n_sample = roi_cls_loc.shape[0]\n",
        "        roi_cls_loc = roi_cls_loc.view(n_sample, -1, 4)\n",
        "        roi_loc = roi_cls_loc[torch.arange(0, n_sample).long().cuda(), \\\n",
        "                              totensor(gt_roi_label).long()]\n",
        "        gt_roi_label = totensor(gt_roi_label).long()\n",
        "        gt_roi_loc = totensor(gt_roi_loc)\n",
        "\n",
        "        # faster rcnn bounding box regression loss\n",
        "        roi_loc_loss = _fast_rcnn_loc_loss(\n",
        "            roi_loc.contiguous(),\n",
        "            gt_roi_loc,\n",
        "            gt_roi_label.data,\n",
        "            self.roi_sigma)\n",
        "\n",
        "        # faster rcnn classification loss\n",
        "        roi_cls_loss = nn.CrossEntropyLoss()(roi_score, gt_roi_label.cuda())\n",
        "\n",
        "        self.roi_cm.add(totensor(roi_score, False), gt_roi_label.data.long())\n",
        "\n",
        "        losses = [rpn_loc_loss, rpn_cls_loss, roi_loc_loss, roi_cls_loss]\n",
        "        losses = losses + [sum(losses)] # total_loss == sum(losses)\n",
        "\n",
        "        return LossTuple(*losses)\n",
        "\n",
        "    # training\n",
        "    def train_step(self, imgs, bboxes, labels, scale):\n",
        "        self.optimizer.zero_grad()\n",
        "        losses = self.forward(imgs, bboxes, labels, scale)\n",
        "        losses.total_loss.backward()\n",
        "        self.optimizer.step()\n",
        "        self.update_meters(losses)\n",
        "        return losses\n",
        "\n",
        "    # checkpoint 만들기\n",
        "    def save(self, save_optimizer=False, save_path=None):\n",
        "        save_dict = dict()\n",
        "\n",
        "        save_dict['model'] = self.faster_rcnn.state_dict()\n",
        "\n",
        "        if save_optimizer:\n",
        "            save_dict['optimizer'] = self.optimizer.state_dict()\n",
        "\n",
        "        if save_path is None:\n",
        "            save_path = './checkpoints/faster_rcnn_scratch_checkpoints.pth'\n",
        "\n",
        "        save_dir = os.path.dirname(save_path)\n",
        "        if not os.path.exists(save_dir):\n",
        "            os.makedirs(save_dir)\n",
        "\n",
        "        torch.save(save_dict, save_path)\n",
        "        return save_path\n",
        "\n",
        "    # checkpoint load\n",
        "    def load(self, path, load_optimizer=True, parse_opt=False, ):\n",
        "        state_dict = torch.load(path)\n",
        "        if 'model' in state_dict:\n",
        "            self.faster_rcnn.load_state_dict(state_dict['model'])\n",
        "        else:  # legacy way, for backward compatibility\n",
        "            self.faster_rcnn.load_state_dict(state_dict)\n",
        "            return self\n",
        "        if 'optimizer' in state_dict and load_optimizer:\n",
        "            self.optimizer.load_state_dict(state_dict['optimizer'])\n",
        "        return self\n",
        "\n",
        "    def update_meters(self, losses):\n",
        "        loss_d = {k: scalar(v) for k, v in losses._asdict().items()}\n",
        "        for key, meter in self.meters.items():\n",
        "            meter.add(loss_d[key])\n",
        "\n",
        "    def reset_meters(self):\n",
        "        for key, meter in self.meters.items():\n",
        "            meter.reset()\n",
        "        self.roi_cm.reset()\n",
        "        self.rpn_cm.reset()\n",
        "\n",
        "    def get_meter_data(self):\n",
        "        return {k: v.value()[0] for k, v in self.meters.items()}\n",
        "\n",
        "\n",
        "def _smooth_l1_loss(x, t, in_weight, sigma):\n",
        "    sigma2 = sigma ** 2\n",
        "    diff = in_weight * (x - t)\n",
        "    abs_diff = diff.abs()\n",
        "    flag = (abs_diff.data < (1. / sigma2)).float()\n",
        "    y = (flag * (sigma2 / 2.) * (diff ** 2) +\n",
        "         (1 - flag) * (abs_diff - 0.5 / sigma2))\n",
        "    return y.sum()\n",
        "\n",
        "\n",
        "def _fast_rcnn_loc_loss(pred_loc, gt_loc, gt_label, sigma):\n",
        "    # Localization loss 구할 때는 positive example에 대해서만 계산\n",
        "    in_weight = torch.zeros(gt_loc.shape).cuda()\n",
        "    in_weight[(gt_label > 0).view(-1, 1).expand_as(in_weight).cuda()] = 1\n",
        "    loc_loss = _smooth_l1_loss(pred_loc, gt_loc, in_weight.detach(), sigma)\n",
        "    loc_loss /= ((gt_label >= 0).sum().float())\n",
        "    return loc_loss\n"
      ],
      "metadata": {
        "id": "hRllVOa48wPS"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Train"
      ],
      "metadata": {
        "id": "LLAleLu786j6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train():\n",
        "    # Train dataset 불러오기\n",
        "#     dataset = TrainDataset()\n",
        "    annotation = os.path.join(data_dir,'train.json')\n",
        "    dataset = TrainCustom(annotation, data_dir, transforms=True)\n",
        "    print('load data')\n",
        "    dataloader = data_.DataLoader(dataset,\n",
        "                                  batch_size=1,     # only batch_size=1 support\n",
        "                                  shuffle=True,\n",
        "                                  pin_memory=False,\n",
        "                                  num_workers=4)\n",
        "\n",
        "    # faster rcnn 불러오기\n",
        "    faster_rcnn = FasterRCNNVGG16().cuda()\n",
        "    print('model construct completed')\n",
        "\n",
        "    # faster rcnn trainer 불러오기\n",
        "    trainer = FasterRCNNTrainer(faster_rcnn).cuda()\n",
        "\n",
        "    # checkpoint load\n",
        "    if train_load_path:\n",
        "        trainer.load(train_load_path)\n",
        "        print('load pretrained model from %s' % train_load_path)\n",
        "\n",
        "    lr_ = learning_rate\n",
        "    best_loss = 1000\n",
        "    for epoch in range(epochs):\n",
        "        trainer.reset_meters()\n",
        "        for ii, (img, bbox_, label_, scale) in enumerate(tqdm(dataloader)):\n",
        "\n",
        "            img, bbox, label = img.cuda().float(), bbox_.cuda(), label_.cuda()\n",
        "            trainer.train_step(img, bbox, label, float(scale))\n",
        "\n",
        "        losses = trainer.get_meter_data()\n",
        "        print(f\"Epoch #{epoch+1} loss: {losses}\")\n",
        "        if losses['total_loss'] < best_loss :\n",
        "            trainer.save()\n",
        "\n",
        "        if epoch == 9:\n",
        "            trainer.faster_rcnn.scale_lr(lr_decay)\n",
        "            lr_ = lr_ * lr_decay\n",
        "\n",
        "        if epoch == 13:\n",
        "            break"
      ],
      "metadata": {
        "id": "1vR_Gmlt87u3"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Inference"
      ],
      "metadata": {
        "id": "9ko1seU18-bI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def eval(dataloader, faster_rcnn):\n",
        "    outputs = []\n",
        "    for ii, (imgs, sizes) in enumerate(tqdm(dataloader)):\n",
        "        sizes = [sizes[0][0].item(), sizes[1][0].item()]\n",
        "        pred_bboxes_, pred_labels_, pred_scores_ = faster_rcnn.predict(imgs, [sizes])\n",
        "        for out in range(len(pred_bboxes_)):\n",
        "            outputs.append({'boxes':pred_bboxes_[out], 'scores': pred_scores_[out], 'labels': pred_labels_[out]})\n",
        "\n",
        "    return outputs"
      ],
      "metadata": {
        "id": "Dz8KVCVe8_aI"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def inference():\n",
        "\n",
        "    # Test dataset 불러오기\n",
        "#     testset = TestDataset()\n",
        "    annotation = os.path.join(data_dir,'test.json')\n",
        "    testset = TestCustom(annotation, data_dir)\n",
        "    test_dataloader = data_.DataLoader(testset,\n",
        "                                       batch_size=1, # only batch_size=1 support\n",
        "                                       num_workers=4,\n",
        "                                       shuffle=False,\n",
        "                                       pin_memory=False\n",
        "                                       )\n",
        "    # faster rcnn 불러오기\n",
        "    faster_rcnn = FasterRCNNVGG16().cuda()\n",
        "    state_dict = torch.load(inf_load_path)\n",
        "    if 'model' in state_dict:\n",
        "        faster_rcnn.load_state_dict(state_dict['model'])\n",
        "    print('load pretrained model from %s' % inf_load_path)\n",
        "\n",
        "    # evaluation\n",
        "    outputs = eval(test_dataloader, faster_rcnn)\n",
        "    score_threshold = 0.05\n",
        "    prediction_strings = []\n",
        "    file_names = []\n",
        "\n",
        "    # submission file 작성\n",
        "    coco = COCO(os.path.join(data_dir, 'test.json'))\n",
        "    for i, output in enumerate(outputs):\n",
        "        prediction_string = ''\n",
        "        image_info = coco.loadImgs(coco.getImgIds(imgIds=i))[0]\n",
        "        for box, score, label in zip(output['boxes'], output['scores'], output['labels']):\n",
        "            if score > score_threshold:\n",
        "                prediction_string += str(label) + ' ' + str(score) + ' ' + str(box[1]) + ' ' + str(\n",
        "                    box[0]) + ' ' + str(box[3]) + ' ' + str(box[2]) + ' '\n",
        "        prediction_strings.append(prediction_string)\n",
        "        file_names.append(image_info['file_name'])\n",
        "    submission = pd.DataFrame()\n",
        "    submission['PredictionString'] = prediction_strings\n",
        "    submission['image_id'] = file_names\n",
        "    submission.to_csv(\"./faster_rcnn_scratch_submission.csv\", index=False)\n",
        "\n",
        "    print(submission.head())"
      ],
      "metadata": {
        "id": "GgOIodjO9HM4"
      },
      "execution_count": 20,
      "outputs": []
    }
  ]
}